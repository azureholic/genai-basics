{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inititalize the notebook\n",
    "import os\n",
    "import textwrap\n",
    "import pymongo\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    ")\n",
    "\n",
    "mongo_user = os.getenv(\"COSMOS_MONGO_USER\")\n",
    "mongo_pwd = os.getenv(\"COSMOS_MONGO_PWD\")\n",
    "mongo_server = os.getenv(\"COSMOS_MONGO_SERVER\")\n",
    "mongo_conn = f\"mongodb+srv://{mongo_user}:{mongo_pwd}@{mongo_server}?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000\"\n",
    "\n",
    "\n",
    "embeddings_deployment_name = \"text-embedding-ada-002\"\n",
    "chat_deployment_name = \"gpt-4o\"\n",
    "\n",
    "print(\"*** init done! ***\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoe ziet een vector er uit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"cat\"\n",
    "\n",
    "embeddings = openai_client.embeddings.create(input = word, model=embeddings_deployment_name)\n",
    "\n",
    "embeddingData = embeddings.data[0].embedding\n",
    "\n",
    "print(\"De vector voor de het woord: \" + word)\n",
    "print(\"Het aantal dimensies in de vector is: \" + str(len(embeddingData)))\n",
    "print(\"Dit zijn de waarden van de vector: \")\n",
    "print(embeddingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergelijk de semantische gelijkheid van woorden\n",
    "\n",
    "We hebben een lijst van woorden en een woord dat we willen vergelijken.\n",
    "\n",
    "* Bereken een vector van ieder woord in de lijst\n",
    "* Bereken een vector van het woord dat we willen vergelijken\n",
    "* Bepaal de gelijkheid (score) met een dotproduct algoritme (er zijn meerdere algoritmes om dit te doen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DotProduct is a simple algorithm to calculate simularity between two vectors\n",
    "### Multiply each element in vector1 with the corresponding element in vector2\n",
    "### Add the results together for each pair of elements\n",
    "### The result is the dot product of the two vectors\n",
    "### https://www.mathsisfun.com/algebra/vectors-dot-product.html\n",
    "\n",
    "def dotproduct(vector1, vector2):\n",
    "    #return sum((a*b) for a, b in zip(vector1, vector2))\n",
    "    #get the length of the vector1 \n",
    "    length = len(vector1)\n",
    "\n",
    "    #check if the length of the vector2 is the same as vector1\n",
    "    if length != len(vector2):\n",
    "        print(\"The vectors are not the same length\")\n",
    "        return\n",
    "\n",
    "    #iterate over the vector based on the length of the vector\n",
    "    sum = 0\n",
    "\n",
    "    for i in range(length):\n",
    "        #multiply the vectors and add them to the sum\n",
    "        sum += vector1[i] * vector2[i]\n",
    "\n",
    "    return sum\n",
    "\n",
    "##### end of the dotproduct function #####\n",
    "\n",
    "words = ['cat', \n",
    "         'hamburger', \n",
    "         'computer', \n",
    "         'server',\n",
    "         'dog', \n",
    "         'pizza', \n",
    "         'laptop', \n",
    "         'horse', \n",
    "         'car',\n",
    "         'steak', \n",
    "         'truck',\n",
    "         'pickup']\n",
    "\n",
    "word = 'food'\n",
    "\n",
    "#create a list of embeddings for each word\n",
    "wordListEmbedding = []\n",
    "for w in words:\n",
    "    response = openai_client.embeddings.create(input = w, model=embeddings_deployment_name).data[0].embedding\n",
    "    wordListEmbedding.append(response)\n",
    "\n",
    "#create an embedding for the word we want to compare\n",
    "wordEmbedding = openai_client.embeddings.create(input = word, model=embeddings_deployment_name).data[0].embedding\n",
    "\n",
    "#calculate the dot product between the word we want to compare and each word in the list\n",
    "dotProductList = []\n",
    "for w in wordListEmbedding:\n",
    "    dotProductList.append(dotproduct(wordEmbedding, w))\n",
    "\n",
    "#sort the dotProductList\n",
    "sortedDotProductList = sorted(dotProductList, reverse=True)\n",
    "\n",
    "#find the index of the words with the highest dot product\n",
    "indexList = []\n",
    "for i in sortedDotProductList:\n",
    "    indexList.append(dotProductList.index(i))\n",
    "\n",
    "#display the top 3 words based on indexlist\n",
    "print(\"De top 3 woorden vergelijkbaar met '\" + word + \"' zijn: \")\n",
    "print(f\"'{words[indexList[0]]}' dotproduct score: + {str(sortedDotProductList[0])}\")\n",
    "print(f\"'{words[indexList[1]]}' dotproduct score: + {str(sortedDotProductList[1])}\")\n",
    "print(f\"'{words[indexList[2]]}' dotproduct score: + {str(sortedDotProductList[2])}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De vector kun je ook berekenen over een zin.\n",
    "\n",
    "In dit geval nemen we user prompt uit het laatste voorbeeld dat we hebben gezien bij \"prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"wanneer is de informatieavond?\"\n",
    "\n",
    "embeddings = openai_client.embeddings.create(input = sentence, model=embeddings_deployment_name)\n",
    "\n",
    "embeddingData = embeddings.data[0].embedding\n",
    "\n",
    "print(\"De vector voor de de zin: \" + sentence)\n",
    "print(\"Het aantal dimensies in de vector is: \" + str(len(embeddingData)))\n",
    "print(\"Dit zijn de waardes van de vector: \")\n",
    "print(embeddingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "laten we eens kijken of we de meest relevante pagina kunnen vinden om de vraag te beantwoorden.\n",
    "\n",
    "* We gaan een vector maken van de vraag\n",
    "* we gaan voor iedere pagina een vector maken\n",
    "* We gaan de meest relevante pagina selecteren en in de prompt meegeven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lees de pagina's van de website\n",
    "page1 = open(\"../pages/home.md\", \"r\").read()\n",
    "page2 = open(\"../pages/informatieavond-5-november.md\", \"r\").read()\n",
    "\n",
    "pages = [page1, page2]\n",
    "\n",
    "myDataQuestion = \"wanneer is de informatieavond?\"\n",
    "\n",
    "pagesListEmbedding = []\n",
    "for p in pages:\n",
    "    response = openai_client.embeddings.create(input = p, model=embeddings_deployment_name).data[0].embedding\n",
    "    pagesListEmbedding.append(response)\n",
    "\n",
    "#create an embedding for the word we want to compare\n",
    "questionEmbedding = openai_client.embeddings.create(input = myDataQuestion, model=embeddings_deployment_name).data[0].embedding\n",
    "\n",
    "#calculate the dot product between the word we want to compare and each word in the list\n",
    "dotProductList = []\n",
    "for p in pagesListEmbedding:\n",
    "    dotProductList.append(dotproduct(questionEmbedding, p))\n",
    "\n",
    "#sort the dotProductList\n",
    "sortedDotProductList = sorted(dotProductList, reverse=True)\n",
    "\n",
    "#find the index of the words with the highest dot product\n",
    "indexList = []\n",
    "for i in sortedDotProductList:\n",
    "    indexList.append(dotProductList.index(i))\n",
    "\n",
    "#augment the prompt with the most relevant page\n",
    "myDataResponse = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": myDataQuestion\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : f\"\"\"\n",
    "                Je bent een behulpzame assistent\n",
    "                Je geeft alleen antwoord op basis van de data die je hebt gekregen\n",
    "                Indien je het antwoord niet kunt vinden, \n",
    "                zeg je dat je het antwoord niet weet\n",
    "                \n",
    "                ## data\n",
    "                {pages[indexList[0]]}\n",
    "\n",
    "                \"\"\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "lines = textwrap.wrap(myDataResponse.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n",
    "print(\"----\")\n",
    "print(\"Hier wat info over de tokens die je hebt gebruikt:\")\n",
    "print(myDataResponse.usage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit is al een stuk beter!\n",
    "\n",
    "Maar hebben we het hele document wel nodig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of files in the chunks directory\n",
    "files = os.listdir(\"../chunks\")\n",
    "pages = []\n",
    "\n",
    "for file in files:\n",
    "    page = open(f\"../chunks/{file}\", \"r\").read()\n",
    "    pages.append(page)\n",
    "\n",
    "pagesListEmbedding = []\n",
    "for p in pages:\n",
    "    response = openai_client.embeddings.create(input = p, model=embeddings_deployment_name).data[0].embedding\n",
    "    pagesListEmbedding.append(response)\n",
    "\n",
    "#create an embedding for the word we want to compare\n",
    "questionEmbedding = openai_client.embeddings.create(input = myDataQuestion, model=embeddings_deployment_name).data[0].embedding\n",
    "\n",
    "#calculate the dot product between the word we want to compare and each word in the list\n",
    "dotProductList = []\n",
    "for p in pagesListEmbedding:\n",
    "    dotProductList.append(dotproduct(questionEmbedding, p))\n",
    "\n",
    "#sort the dotProductList\n",
    "sortedDotProductList = sorted(dotProductList, reverse=True)\n",
    "\n",
    "#find the index of the words with the highest dot product\n",
    "indexList = []\n",
    "for i in sortedDotProductList:\n",
    "    indexList.append(dotProductList.index(i))\n",
    "\n",
    "#augment the prompt with the most relevant page\n",
    "myDataResponse = openai_client.chat.completions.create(\n",
    "    model = chat_deployment_name,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": myDataQuestion\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\" : f\"\"\"\n",
    "                Je bent een behulpzame assistent\n",
    "                Je geeft alleen antwoord op basis van de data die je hebt gekregen\n",
    "                Indien je het antwoord niet kunt vinden, \n",
    "                zeg je dat je het antwoord niet weet\n",
    "                \n",
    "                ## data\n",
    "                {pages[indexList[0]]}\n",
    "\n",
    "                \"\"\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "lines = textwrap.wrap(myDataResponse.choices[0].message.content, width=80)\n",
    "for line in lines:\n",
    "    print(line)\n",
    "\n",
    "print(\"----\")\n",
    "print(\"Hier wat info over de tokens die je hebt gebruikt:\")\n",
    "print(myDataResponse.usage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weer een verbetering! Minder tokens en toch een antwoord!\n",
    "\n",
    "Embeddings berekenen kost echter ook tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_client.embeddings.create(input = pages[indexList[0]], model=embeddings_deployment_name)\n",
    "response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectoren berekenen hoeven we niet iedere keer te doen.\n",
    "\n",
    "We kunnen de vectoren ook opslaan en hergebruiken.\n",
    "\n",
    "Hier hebben we een vector database voor nodig. \n",
    "\n",
    "Er zijn verschillende vector databases. \n",
    "\n",
    "We kiezen hier voor MongoDb. In dit geval gebruiken Cosine Simularity als algoritme. Dit algoritme is ingebouwd in de database om gelijkheid te berekenen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseName = \"vectorsample\"\n",
    "collectionName = \"chunks\"\n",
    "\n",
    "mongo_client = pymongo.MongoClient(mongo_conn)\n",
    "db = mongo_client[databaseName]\n",
    "collection = db[collectionName]\n",
    "\n",
    "if collectionName not in db.list_collection_names():\n",
    "    db.create_collection(collectionName)\n",
    "else:\n",
    "    #delete all documents in the collection so we can rerun this script\n",
    "    collection.delete_many({})\n",
    "\n",
    "\n",
    "db.command({\n",
    "  'createIndexes': f'{collectionName}',\n",
    "  'indexes': [\n",
    "    {\n",
    "      'name': 'VectorSearchIndex',\n",
    "      'key': {\n",
    "        \"contentVector\": \"cosmosSearch\"\n",
    "      },\n",
    "      'cosmosSearchOptions': {\n",
    "        'kind': 'vector-ivf',\n",
    "        'numLists': 1,\n",
    "        'similarity': 'COS',\n",
    "        'dimensions': 1536\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "\n",
    "files = os.listdir(\"../chunks\")\n",
    "pages = []\n",
    "filesUploaded = 0\n",
    "for file in files:\n",
    "    page = open(f\"../chunks/{file}\", \"r\").read()\n",
    "    response = openai_client.embeddings.create(input = page, model=embeddings_deployment_name).data[0].embedding\n",
    "    \n",
    "    filetoStore = {\n",
    "        \"filename\": file,\n",
    "        \"content\": page,\n",
    "        \"contentVector\": response\n",
    "    }\n",
    "\n",
    "    collection.insert_one(filetoStore)\n",
    "    filesUploaded += 1\n",
    "\n",
    "print(f\"{filesUploaded} bestanden en vectoren opgeslagen in de database\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen nu een database query uitvoeren met de vector van de vraag als input (dit is geen SQL query, maar een vector search opdracht).\n",
    "\n",
    "De output is een lijst met relevante documenten die in de database zijn opgeslagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(vector, num_results):\n",
    "    pipeline = [\n",
    "        {\n",
    "            '$search': {\n",
    "                \"cosmosSearch\": {\n",
    "                    \"vector\": vector,\n",
    "                    \"path\": \"contentVector\",\n",
    "                    \"k\": num_results \n",
    "                },\n",
    "                \"returnStoredSource\": True }},\n",
    "        {'$project': { 'similarityScore': { '$meta': 'searchScore' }, 'document' : '$$ROOT' } }\n",
    "    ]\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return results\n",
    "\n",
    "myDataQuestion = \"wanneer is de informatieavond?\"\n",
    "#create an embedding for the question we want to compare\n",
    "questionEmbedding = openai_client.embeddings.create(input = myDataQuestion, model=embeddings_deployment_name).data[0].embedding\n",
    "\n",
    "#zoek 3 relevante pagina's op basis van de vraag\n",
    "results = vector_search(questionEmbedding, 3)\n",
    "\n",
    "print(\"De top 3 resultaten zijn: \")\n",
    "for result in results:\n",
    "    print(result[\"document\"][\"filename\"] + \" - \" + str(result[\"similarityScore\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
